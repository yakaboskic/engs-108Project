{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.7 (default, Oct 25 2018, 09:16:13) \\n[GCC 5.4.0 20160609]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, AveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SOURCES = '/home/yakaboskic/src/'\n",
    "MODULE_FOLDER_NAMES = ['Design-of-experiment-Python']\n",
    "\n",
    "for module in MODULE_FOLDER_NAMES:\n",
    "    mod_path = os.path.join(PATH_TO_SOURCES, module)\n",
    "    if not sys.path.__contains__(mod_path):\n",
    "        sys.path.append(mod_path)\n",
    "    \n",
    "import DOE_functions\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1_path = 'data/cifar-10-batches-py/data_batch_1'\n",
    "dict_1 = unpickle(batch_1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = copy.deepcopy(dict_1[b'data'])/255.0\n",
    "y_train = np.array(copy.deepcopy(dict_1[b'labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(10000, 32, 32, 3)\n",
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Build CNN Model\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv2D(input_shape= x_train[0].shape, \n",
    "                                  filters=32, \n",
    "                                  kernel_size=(10,10), \n",
    "                                  strides = (1,1),\n",
    "                                  padding='Same', \n",
    "                                  activation='relu'))\n",
    "\n",
    "model_cnn.add(MaxPool2D(pool_size=(2,2)))\n",
    "model_cnn.add(Dropout(0.25))\n",
    "\n",
    "model_cnn.add(Conv2D(filters=64, \n",
    "                                  kernel_size=(3,3), \n",
    "                                  padding='Same', \n",
    "                                  activation='relu'))\n",
    "\n",
    "model_cnn.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model_cnn.add(Dropout(0.25))\n",
    "\n",
    "model_cnn.add(Conv2D(filters=64, \n",
    "                                  kernel_size=(5,5), \n",
    "                                  padding='Same', \n",
    "                                  activation='relu'))\n",
    "\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(256, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 32)          896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 1, 1, 64)          102464    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 141,066\n",
      "Trainable params: 141,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python optimisation variables\n",
    "epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "#-- Set up the Optimizer\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Compile the Model\n",
    "model_cnn.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 1s 126us/step - loss: 2.2288 - acc: 0.1510\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 2.1446 - acc: 0.1770\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 2.1034 - acc: 0.2074\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 2.0650 - acc: 0.2273\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 2.0472 - acc: 0.2350\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 2.0310 - acc: 0.2448\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 1s 112us/step - loss: 2.0227 - acc: 0.2487\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 1s 113us/step - loss: 2.0100 - acc: 0.2550\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 1s 113us/step - loss: 1.9993 - acc: 0.2632\n",
      "Epoch 10/20\n",
      "   50/10000 [..............................] - ETA: 1s - loss: 1.9255 - acc: 0.3000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-17dd9755241b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history_cnn = model_cnn.fit(x_train, y_train, \n\u001b[1;32m      2\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                             epochs=epochs)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_cnn = model_cnn.fit(x_train, y_train, \n",
    "                            batch_size=batch_size, \n",
    "                            epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history_cnn.history['loss'], color='b', label='Training Loss')\n",
    "#ax[0].plot(history_cnn.history['val_loss'], color='r', label='Validation Loss', axes=ax[0])\n",
    "legend= ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history_cnn.history['acc'], color='b', label='Training Accuracy')\n",
    "#ax[1].plot(history_cnn.history['val_acc'], color='r', label='Validation Accuracy')\n",
    "legend= ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_path = 'data/cifar-10-batches-py/test_batch'\n",
    "dict_test = unpickle(test_batch_path)\n",
    "x_test = copy.deepcopy(dict_test[b'data'])/255.0\n",
    "y_test = np.array(copy.deepcopy(dict_test[b'labels']))\n",
    "x_test = x_test.reshape(10000, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 410us/step\n",
      "1.343236236190796 0.5655\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_cnn.evaluate(x_test, y_test)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_activation1 had more than two levels. Assigning the end point to the high level.\n",
      "pool_type1 had more than two levels. Assigning the end point to the high level.\n"
     ]
    }
   ],
   "source": [
    "dict_values = {'conv_filters1': [10,80],\n",
    "              'conv_kernelSize1': [2,10],\n",
    "              'conv_strides1': [1,10],\n",
    "              'conv_activation1': [1,2,3],\n",
    "              #'conv_filters2': [10,80],\n",
    "              #'conv_kernelSize2': [2,10],\n",
    "              #'conv_strides2': [1,10],\n",
    "              #'conv_activation2': [1,2,3],\n",
    "              #'conv_filters3': [10,80],\n",
    "              #'conv_kernelSize3': [2,10],\n",
    "              #'conv_strides3': [1,10],\n",
    "              #'conv_activation3': [1,2,3],\n",
    "              'pool_type1': [1,2,3],\n",
    "              'pool_size1': [2,10],\n",
    "              'pool_strides1': [1, 10],\n",
    "              'pool_momentum1': [.5, .95],\n",
    "              #'pool_type2': [1,2,3],\n",
    "              #'pool_size2': [2,10],\n",
    "              #'pool_strides2': [1, 10],\n",
    "              #'pool_momentum2': [.5, .95],\n",
    "              #'pool_type3': [1,2,3],\n",
    "              #'pool_size3': [2,10],\n",
    "              #'pool_strides3': [1, 10],\n",
    "              #'pool_momentum3': [.5, .95],\n",
    "              'drop_value1': [0, .5],\n",
    "              #'drop_value2': [0, .5],\n",
    "              #'drop_value3': [0, .5],\n",
    "              'dense_num' : [100, 1000]}\n",
    "\n",
    "\n",
    "doe = DOE_functions.build_central_composite(dict_values, face='ccf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "(item1, item2) = doe.loc[0,['conv_filters1', 'dense_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_filters1</th>\n",
       "      <th>conv_kernelSize1</th>\n",
       "      <th>conv_strides1</th>\n",
       "      <th>conv_activation1</th>\n",
       "      <th>pool_type1</th>\n",
       "      <th>pool_size1</th>\n",
       "      <th>pool_strides1</th>\n",
       "      <th>pool_momentum1</th>\n",
       "      <th>drop_value1</th>\n",
       "      <th>dense_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>80.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>80.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>45.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.00</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.50</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>550.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      conv_filters1  conv_kernelSize1  conv_strides1  conv_activation1  \\\n",
       "0              10.0               2.0            1.0               1.0   \n",
       "1              80.0               2.0            1.0               1.0   \n",
       "2              10.0              10.0            1.0               1.0   \n",
       "3              80.0              10.0            1.0               1.0   \n",
       "4              10.0               2.0           10.0               1.0   \n",
       "5              80.0               2.0           10.0               1.0   \n",
       "6              10.0              10.0           10.0               1.0   \n",
       "7              80.0              10.0           10.0               1.0   \n",
       "8              10.0               2.0            1.0               3.0   \n",
       "9              80.0               2.0            1.0               3.0   \n",
       "10             10.0              10.0            1.0               3.0   \n",
       "11             80.0              10.0            1.0               3.0   \n",
       "12             10.0               2.0           10.0               3.0   \n",
       "13             80.0               2.0           10.0               3.0   \n",
       "14             10.0              10.0           10.0               3.0   \n",
       "15             80.0              10.0           10.0               3.0   \n",
       "16             10.0               2.0            1.0               1.0   \n",
       "17             80.0               2.0            1.0               1.0   \n",
       "18             10.0              10.0            1.0               1.0   \n",
       "19             80.0              10.0            1.0               1.0   \n",
       "20             10.0               2.0           10.0               1.0   \n",
       "21             80.0               2.0           10.0               1.0   \n",
       "22             10.0              10.0           10.0               1.0   \n",
       "23             80.0              10.0           10.0               1.0   \n",
       "24             10.0               2.0            1.0               3.0   \n",
       "25             80.0               2.0            1.0               3.0   \n",
       "26             10.0              10.0            1.0               3.0   \n",
       "27             80.0              10.0            1.0               3.0   \n",
       "28             10.0               2.0           10.0               3.0   \n",
       "29             80.0               2.0           10.0               3.0   \n",
       "...             ...               ...            ...               ...   \n",
       "1018           10.0              10.0            1.0               3.0   \n",
       "1019           80.0              10.0            1.0               3.0   \n",
       "1020           10.0               2.0           10.0               3.0   \n",
       "1021           80.0               2.0           10.0               3.0   \n",
       "1022           10.0              10.0           10.0               3.0   \n",
       "1023           80.0              10.0           10.0               3.0   \n",
       "1024           45.0               6.0            5.5               2.0   \n",
       "1025           45.0               6.0            5.5               2.0   \n",
       "1026           10.0               6.0            5.5               2.0   \n",
       "1027           80.0               6.0            5.5               2.0   \n",
       "1028           45.0               2.0            5.5               2.0   \n",
       "1029           45.0              10.0            5.5               2.0   \n",
       "1030           45.0               6.0            1.0               2.0   \n",
       "1031           45.0               6.0           10.0               2.0   \n",
       "1032           45.0               6.0            5.5               1.0   \n",
       "1033           45.0               6.0            5.5               3.0   \n",
       "1034           45.0               6.0            5.5               2.0   \n",
       "1035           45.0               6.0            5.5               2.0   \n",
       "1036           45.0               6.0            5.5               2.0   \n",
       "1037           45.0               6.0            5.5               2.0   \n",
       "1038           45.0               6.0            5.5               2.0   \n",
       "1039           45.0               6.0            5.5               2.0   \n",
       "1040           45.0               6.0            5.5               2.0   \n",
       "1041           45.0               6.0            5.5               2.0   \n",
       "1042           45.0               6.0            5.5               2.0   \n",
       "1043           45.0               6.0            5.5               2.0   \n",
       "1044           45.0               6.0            5.5               2.0   \n",
       "1045           45.0               6.0            5.5               2.0   \n",
       "1046           45.0               6.0            5.5               2.0   \n",
       "1047           45.0               6.0            5.5               2.0   \n",
       "\n",
       "      pool_type1  pool_size1  pool_strides1  pool_momentum1  drop_value1  \\\n",
       "0            1.0         2.0            1.0           0.500         0.00   \n",
       "1            1.0         2.0            1.0           0.500         0.00   \n",
       "2            1.0         2.0            1.0           0.500         0.00   \n",
       "3            1.0         2.0            1.0           0.500         0.00   \n",
       "4            1.0         2.0            1.0           0.500         0.00   \n",
       "5            1.0         2.0            1.0           0.500         0.00   \n",
       "6            1.0         2.0            1.0           0.500         0.00   \n",
       "7            1.0         2.0            1.0           0.500         0.00   \n",
       "8            1.0         2.0            1.0           0.500         0.00   \n",
       "9            1.0         2.0            1.0           0.500         0.00   \n",
       "10           1.0         2.0            1.0           0.500         0.00   \n",
       "11           1.0         2.0            1.0           0.500         0.00   \n",
       "12           1.0         2.0            1.0           0.500         0.00   \n",
       "13           1.0         2.0            1.0           0.500         0.00   \n",
       "14           1.0         2.0            1.0           0.500         0.00   \n",
       "15           1.0         2.0            1.0           0.500         0.00   \n",
       "16           3.0         2.0            1.0           0.500         0.00   \n",
       "17           3.0         2.0            1.0           0.500         0.00   \n",
       "18           3.0         2.0            1.0           0.500         0.00   \n",
       "19           3.0         2.0            1.0           0.500         0.00   \n",
       "20           3.0         2.0            1.0           0.500         0.00   \n",
       "21           3.0         2.0            1.0           0.500         0.00   \n",
       "22           3.0         2.0            1.0           0.500         0.00   \n",
       "23           3.0         2.0            1.0           0.500         0.00   \n",
       "24           3.0         2.0            1.0           0.500         0.00   \n",
       "25           3.0         2.0            1.0           0.500         0.00   \n",
       "26           3.0         2.0            1.0           0.500         0.00   \n",
       "27           3.0         2.0            1.0           0.500         0.00   \n",
       "28           3.0         2.0            1.0           0.500         0.00   \n",
       "29           3.0         2.0            1.0           0.500         0.00   \n",
       "...          ...         ...            ...             ...          ...   \n",
       "1018         3.0        10.0           10.0           0.950         0.50   \n",
       "1019         3.0        10.0           10.0           0.950         0.50   \n",
       "1020         3.0        10.0           10.0           0.950         0.50   \n",
       "1021         3.0        10.0           10.0           0.950         0.50   \n",
       "1022         3.0        10.0           10.0           0.950         0.50   \n",
       "1023         3.0        10.0           10.0           0.950         0.50   \n",
       "1024         2.0         6.0            5.5           0.725         0.25   \n",
       "1025         2.0         6.0            5.5           0.725         0.25   \n",
       "1026         2.0         6.0            5.5           0.725         0.25   \n",
       "1027         2.0         6.0            5.5           0.725         0.25   \n",
       "1028         2.0         6.0            5.5           0.725         0.25   \n",
       "1029         2.0         6.0            5.5           0.725         0.25   \n",
       "1030         2.0         6.0            5.5           0.725         0.25   \n",
       "1031         2.0         6.0            5.5           0.725         0.25   \n",
       "1032         2.0         6.0            5.5           0.725         0.25   \n",
       "1033         2.0         6.0            5.5           0.725         0.25   \n",
       "1034         1.0         6.0            5.5           0.725         0.25   \n",
       "1035         3.0         6.0            5.5           0.725         0.25   \n",
       "1036         2.0         2.0            5.5           0.725         0.25   \n",
       "1037         2.0        10.0            5.5           0.725         0.25   \n",
       "1038         2.0         6.0            1.0           0.725         0.25   \n",
       "1039         2.0         6.0           10.0           0.725         0.25   \n",
       "1040         2.0         6.0            5.5           0.500         0.25   \n",
       "1041         2.0         6.0            5.5           0.950         0.25   \n",
       "1042         2.0         6.0            5.5           0.725         0.00   \n",
       "1043         2.0         6.0            5.5           0.725         0.50   \n",
       "1044         2.0         6.0            5.5           0.725         0.25   \n",
       "1045         2.0         6.0            5.5           0.725         0.25   \n",
       "1046         2.0         6.0            5.5           0.725         0.25   \n",
       "1047         2.0         6.0            5.5           0.725         0.25   \n",
       "\n",
       "      dense_num  \n",
       "0         100.0  \n",
       "1         100.0  \n",
       "2         100.0  \n",
       "3         100.0  \n",
       "4         100.0  \n",
       "5         100.0  \n",
       "6         100.0  \n",
       "7         100.0  \n",
       "8         100.0  \n",
       "9         100.0  \n",
       "10        100.0  \n",
       "11        100.0  \n",
       "12        100.0  \n",
       "13        100.0  \n",
       "14        100.0  \n",
       "15        100.0  \n",
       "16        100.0  \n",
       "17        100.0  \n",
       "18        100.0  \n",
       "19        100.0  \n",
       "20        100.0  \n",
       "21        100.0  \n",
       "22        100.0  \n",
       "23        100.0  \n",
       "24        100.0  \n",
       "25        100.0  \n",
       "26        100.0  \n",
       "27        100.0  \n",
       "28        100.0  \n",
       "29        100.0  \n",
       "...         ...  \n",
       "1018     1000.0  \n",
       "1019     1000.0  \n",
       "1020     1000.0  \n",
       "1021     1000.0  \n",
       "1022     1000.0  \n",
       "1023     1000.0  \n",
       "1024      550.0  \n",
       "1025      550.0  \n",
       "1026      550.0  \n",
       "1027      550.0  \n",
       "1028      550.0  \n",
       "1029      550.0  \n",
       "1030      550.0  \n",
       "1031      550.0  \n",
       "1032      550.0  \n",
       "1033      550.0  \n",
       "1034      550.0  \n",
       "1035      550.0  \n",
       "1036      550.0  \n",
       "1037      550.0  \n",
       "1038      550.0  \n",
       "1039      550.0  \n",
       "1040      550.0  \n",
       "1041      550.0  \n",
       "1042      550.0  \n",
       "1043      550.0  \n",
       "1044      100.0  \n",
       "1045     1000.0  \n",
       "1046      550.0  \n",
       "1047      550.0  \n",
       "\n",
       "[1048 rows x 10 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doe.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 10)        130       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 16, 16, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           multiple                  410       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 8, 8, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               64100     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 65,650\n",
      "Trainable params: 65,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 3s 278us/step - loss: 2.3226 - acc: 0.0984\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 2.3109 - acc: 0.0990\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 2s 228us/step - loss: 2.3068 - acc: 0.1011\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 2s 227us/step - loss: 2.3028 - acc: 0.0990\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 2.3025 - acc: 0.0981\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 2s 232us/step - loss: 2.3024 - acc: 0.0995\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 2s 228us/step - loss: 2.3024 - acc: 0.1021\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 2.3023 - acc: 0.1007\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 2s 231us/step - loss: 2.3023 - acc: 0.1021\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 2s 226us/step - loss: 2.3023 - acc: 0.1015\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 2s 227us/step - loss: 2.3023 - acc: 0.1006 0s - loss: 2.3026 - acc: 0.\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 2.3023 - acc: 0.1000\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 2s 228us/step - loss: 2.3023 - acc: 0.1007\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 2s 241us/step - loss: 2.3023 - acc: 0.1032\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 2s 227us/step - loss: 2.3023 - acc: 0.1003\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 2s 226us/step - loss: 2.3023 - acc: 0.0977\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 2s 225us/step - loss: 2.3023 - acc: 0.1004\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 2.3023 - acc: 0.1030\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 2.3024 - acc: 0.0967\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 2s 225us/step - loss: 2.3023 - acc: 0.0996\n",
      "10000/10000 [==============================] - 2s 155us/step\n"
     ]
    }
   ],
   "source": [
    "act_map = {1:'sigmoid', 2:'relu', 3:'tanh'}\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "results = []\n",
    "\n",
    "for test in range(0, 1): #doe.shape[0]):\n",
    "    try:\n",
    "        (conv_filter,\n",
    "         kernel_size,\n",
    "         conv_stride,\n",
    "         activition,\n",
    "         pool_type,\n",
    "         pool_size,\n",
    "         pool_stride,\n",
    "         pool_momentum,\n",
    "         drop_value,\n",
    "         dense_num) = doe.loc[test,:]\n",
    "\n",
    "        if pool_type == 1:\n",
    "            pool_layer = MaxPool2D(padding='Same',\n",
    "                                   strides= (round(pool_size), round(pool_size)),\n",
    "                                   pool_size=(round(pool_size), round(pool_size)))\n",
    "        elif pool_type == 2:\n",
    "            pool_layer = AveragePooling2D(padding='Same',\n",
    "                                   strides= (round(pool_size), round(pool_size)),\n",
    "                                   pool_size=(round(pool_size), round(pool_size)))\n",
    "        else:\n",
    "            pool_layer = BatchNormalization(momentum=pool_momentum)\n",
    "\n",
    "        conv_layer1 = Conv2D(round(conv_filter),\n",
    "                         input_shape = x_train[0].shape,\n",
    "                         activation= act_map[activition],\n",
    "                         kernel_size = (round(kernel_size), round(kernel_size)),\n",
    "                         padding='Same',\n",
    "                         strides = (round(conv_stride), round(conv_stride)))\n",
    "        conv_layer = Conv2D(round(conv_filter),\n",
    "                         activation= act_map[activition],\n",
    "                         kernel_size = (round(kernel_size), round(kernel_size)),\n",
    "                         padding='Same',\n",
    "                         strides = (round(conv_stride), round(conv_stride)))\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(conv_layer1)\n",
    "        model.add(pool_layer)\n",
    "        model.add(Dropout(drop_value))\n",
    "        model.add(conv_layer)\n",
    "        model.add(pool_layer)\n",
    "        model.add(Dropout(drop_value))\n",
    "        model.add(conv_layer)\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(int(dense_num), activation='relu'))\n",
    "        model.add(Dropout(drop_value))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        history = model.fit(x_train, y_train, \n",
    "                                batch_size=batch_size, \n",
    "                                epochs=epochs)\n",
    "        result_loss, result_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "        results.append((result_loss, result_acc))\n",
    "    except Exception,e:\n",
    "        results.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.302966154098511, 0.1)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
